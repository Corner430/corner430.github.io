---
title: 邱锡鹏 《神经网络与深度学习》笔记
date: 2023-06-23 11:42:44
tags:
declare: true
top: 1
---
[电子版书籍](https://nndl.github.io/)<!--more-->
# 第4章 前馈神经网络
1 净输入也叫**净活性值**
![20230623120908](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623120908.png)
![20230623120925](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623120925.png)

2 激活函数需要具备以下几点性质：
  1. **连续并可导(允许少数点上不可导)的非线性函数**。可导的激活函数可以直接利用数值优化的方法来学习网络参数.
  2. **激活函数及其导函数要尽可能的简单**，有利于提高网络计算效率.
  3. 激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性.

3 常用的激活函数
  1. **Sigmoid型函数**是指一类S型曲线函数，为两端饱和函数
     1. Logistic函数：$\sigma(x) = \frac{1}{1 + exp(-x)}$
     2. Tanh函数：$tanh(x) = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}$ = $2\sigma(2x) - 1$
对于函数$f(x)$，若$x→-∞$时，其导数$f'(x)→0$，则称其为左饱和。若$x→+∞$时，其导数$f'(x)→0$，则称其为右饱和。当同时满足左、右饱和时，就称为两端饱和.
![20230623122222](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623122222.png)
> Tanh函数的输出是零中心化的（Zero-Centered），而Logistic函数的输出恒大于0。非零中心化的输出会使得其后一层的神经元的输入发生偏置偏移（Bias Shift），并进一步使得梯度下降的收敛速度变慢。

Logistic函数和Tanh函数都是Sigmoid型函数，具有饱和性，但是**计算开销较大**。**因为这两个函数都是在中间（0附近近似线性，两端饱和。因此，这两个函数可以通过分段函数来近似）**
![20230623122904](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623122904.png)
  2. **RELU函数**
![20230623123043](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623123043.png)
**为解决ReLU带来的问题，例如：偏置偏移、死亡RELU问题等**。有如下ReLU变种：
- 带泄露的ReLU
- 带参数的ReLU
- ELU函数
- Softplus函数
![20230623123637](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623123637.png)
  3. Swish函数是一种**自门控（Self-Gated）**激活函数
![20230623124008](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623124008.png)
  4. GELU函数（Gaussian Error Linear Unit，高斯误差线性单元）也是一种通过门控机制来调整其输出值的激活函数，和Swish函数比较类似。
  5. Maxout单元也是一种分段线性函数。Sigmoid型函数、ReLU等激活函数的输入是神经元的净输入z，是一个标量。**而Maxout单元的输入是上一层神经元的全部原始输出，是一个向量** x = [$x_1;x_2;...;x_D$]

4 常用的三种神经网络结构
- 前馈网络
- 记忆网络
- 图网络
图网络是定义在图结构数据上的神经网络。图中每个节点都由**一个或一组神经元构成**。节点之间的连接可以是有向的，也可以是无向的。**每个节点可以收到来自相邻节点或自身的信息**。图网络是前馈网络和记忆网络的泛化。包含很多不同的实现方式。
![20230623150310](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623150310.png)
> 其中圆形节点表示一个神经元，方向节点表示一组神经元。

5 前馈神经网络也经常被称为**多层感知器（Multi-Layer Perceptron,MLP）**.但多层感知器的叫法**并不是十分合理**，因为前馈神经网络其实是由多层的Logistic回归模型（连续的非线性函数）组成，而不是由多层的感知器（不连续的非线性函数）组成。
![20230623151516](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623151516.png)
> 层数L一般只考虑隐藏层和输出层

6 **前馈神经网络的记号**
![20230623152955](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623152955.png)
![20230623153101](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623153101.png)
- **仿射变换（Affine Transformation）是指在几何空间中对对象进行线性变换和平移的组合操作**。它是一种保持直线平行性、保持点的共线性和保持中点的位置关系的变换。

- 在二维空间中，仿射变换由以下几个基本操作组成：
  - 平移（Translation）：通过添加一个平移向量，将对象沿着指定方向移动一定的距离。
  - 缩放（Scaling）：通过乘以一个缩放因子，改变对象的尺寸大小，可以使对象放大或缩小。
  - 旋转（Rotation）：围绕指定的旋转中心点，按照一定的角度进行旋转操作。
  - 剪切（Shear）：通过斜向拉伸或压缩对象，改变对象的形状。

- 在三维空间中，仿射变换的基本操作与二维空间类似，同时还包括以下操作：
  - 三维平移：对象沿着指定的方向移动一定的距离。
  - 三维缩放：对象在三个坐标轴方向上进行尺寸的缩放。
  - 三维旋转：围绕指定的旋转中心点，按照一定的角度绕任意轴进行旋转。
  - 三维错切：通过斜向拉伸或压缩对象，改变对象的形状。

> 仿射变换在计算机图形学、计算机视觉和几何建模等领域广泛应用。它可以用于图像处理中的平移、旋转和缩放操作，还可以用于计算机动画中的对象变换和相机视角变换等。
> 一言以蔽之，**矩阵运算**

7 通用近似定理（Universal Approximation Theorem）
![20230623153819](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623153819.png)
根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的数量足够，它可以**以任意的精度**来近似任何一个定义在实数空间$\mathbb{R}^D$中的有界闭集函数。
> 通用近似定理知识说明了神经网络的计算能力可以去近似一个给定的连续函数，但并没有给出如何找到这样一个网络，以及是否是最优的。

8 **输入样本的特征对分类器的影响很大**。因此，要取得好的分类效果，需要将样本的原始特征向量$\boldsymbol{x}$转换到更有效的特征向量$\phi{x}$，这个过程叫做**特征抽取**。多层前馈神经网络也可以看成是一种特征转换方法，其输出$\phi{x}$作为分类器的输入进行分类。
![20230623162736](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623162736.png)
> 反之，Logistic回归或Softmax回归也可以看作只有一层的神经网络

9 **One-hot向量是一种用于表示离散分类变量的编码方法。它是一个稀疏向量，其中只有一个元素为1，其他元素都为0。这个元素的位置表示了该变量所属的类别。**

10 交叉熵函数（Cross-Entropy Function）是一种在信息论和机器学习中常用的损失函数，**用于衡量两个概率分布之间的差异**。它广泛应用于分类任务和神经网络的训练中。
假设我们有一个真实的概率分布P和一个模型的输出概率分布Q，交叉熵函数衡量了Q与P之间的差异。
交叉熵函数可以解释为信息的平均编码长度。**当P和Q的分布越接近，交叉熵值越小，表示模型的预测越准确。当两个分布完全一致时，交叉熵达到最小值为0。**

11 TODO

---------------------------------
# 第5章 卷积神经网络
1 卷积神经网络（Convolutional Neural Network,CNN或ConvNet）是一种具有**局部连接、权重共享**等特性的深层前馈神经网络
2 全连接前馈网络无法处理图像，卷积神经网络应运而生。
  1. 参数太多
  2. 局部不变性特征
3 在计算机视觉和深度学习领域，**感受野（Receptive Field）指的是在输入图像上的一个区域，对应于输出特征图上的一个单元或神经元。感受野表示了该神经元对输入图像的可见区域大小**。

在卷积神经网络（CNN）中，每个神经元或特征图单元都与输入图像上的一个局部区域连接，并使用卷积运算从该区域提取特征。感受野的大小决定了神经元能够感知到输入图像的区域范围。

感受野的大小可以通过以下方式来计算：
  - 对于输入图像的第一层特征图，感受野的大小通常由卷积核的大小决定。
  - 对于后续的层，感受野的大小取决于前一层的感受野大小以及卷积层的步幅和卷积核的大小。
在卷积神经网络中，**随着网络层的叠加，感受野会逐渐增大**。较小的感受野通常用于捕捉局部细节和纹理等低级特征，而较大的感受野可以用于捕捉更大范围的上下文信息和全局结构特征。

感受野的概念对于理解卷积神经网络中信息传递和特征提取的范围至关重要。**通过增加网络的深度和层数，感受野的范围可以覆盖更广阔的图像区域**，有助于网络学习更复杂和抽象的特征。

4 目前的卷积神经网络一般是由**卷积层、汇聚层和全连接层交叉堆叠而成的前馈神经网络**.卷积神经网络由三个结构上的特性：**局部连接、权重共享以及汇聚**。这些特性使得卷积神经网络具有一定程度上的**平移、缩放和旋转不变性**。

5 一维卷积
![20230623200144](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623200144.png)
![20230623200220](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623200220.png)
> 下层为输入信号序列，上层为卷积结果，连接边上的数字为滤波器中的权重。左图的卷积结果为近似值

6 二维卷积
![20230623200733](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623200733.png)
> 根据卷积的定义，左图的计算需要进行**卷积核翻转**
> 翻转指从两个维度（从上到下、从左到右）颠倒次序，即旋转180度

7 在图像处理中，卷积经常作为特征提取的有效方法。一副图像在经过卷积操作后得到结果称为特征映射（Feature Map）。如下为几个常用滤波器，图中最上面的滤波器是常用的**高斯滤波器**，可以用来对图像进行平滑去噪；中间和最下面的滤波器可以用来提取边缘特征。
![20230623201456](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623201456.png)

8 互相关和卷积的区别
![20230623201810](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623201810.png)

9 在神经网络中使用卷积**是为了进行特征抽取，卷积核是否进行翻转和其特征抽取的能力无关，故而二者在能力上是等价的**

10 在卷积的标准定义基础上，还可以引入卷积核的滑动**步长**和**零填充**来增加卷积的多样性，可以更灵活地进行特征抽取。
  - **步长（Stride）**是指卷积核在滑动时地时间间隔
  - **零填充（Zero Padding）**是在输入向量**两端**进行补零

11 常用的卷积
![20230623212207](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623212207.png)

12 卷积拥有很好地数学性质
  - 交换性
![20230623212557](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623212557.png)
  - 导数
![20230623212706](https://cdn.jsdelivr.net/gh/Corner430/Picture1/images/20230623212706.png)

13   